{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3408409d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from collections import defaultdict\n",
    "from Transaction import Transaction\n",
    "from operator import add\n",
    "from pEFIM import pEFIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1047f6e7",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot run multiple SparkContexts at once; existing SparkContext(app=PEFIM, master=local[*]) created by __init__ at /tmp/ipykernel_69/461506957.py:4 ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [12]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m APP_NAME \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPEFIM\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      3\u001b[0m conf \u001b[38;5;241m=\u001b[39m SparkConf()\u001b[38;5;241m.\u001b[39msetAppName(APP_NAME)\n\u001b[0;32m----> 4\u001b[0m sc \u001b[38;5;241m=\u001b[39m \u001b[43mSparkContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/context.py:144\u001b[0m, in \u001b[0;36mSparkContext.__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m gateway \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m gateway\u001b[38;5;241m.\u001b[39mgateway_parameters\u001b[38;5;241m.\u001b[39mauth_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    141\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are trying to pass an insecure Py4j gateway to Spark. This\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    142\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is not allowed as it is a security risk.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 144\u001b[0m \u001b[43mSparkContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ensure_initialized\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgateway\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgateway\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    146\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n\u001b[1;32m    147\u001b[0m                   conf, jsc, profiler_cls)\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/context.py:350\u001b[0m, in \u001b[0;36mSparkContext._ensure_initialized\u001b[0;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[1;32m    347\u001b[0m     callsite \u001b[38;5;241m=\u001b[39m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context\u001b[38;5;241m.\u001b[39m_callsite\n\u001b[1;32m    349\u001b[0m     \u001b[38;5;66;03m# Raise error if there is already a running Spark context\u001b[39;00m\n\u001b[0;32m--> 350\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    351\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot run multiple SparkContexts at once; \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    352\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexisting SparkContext(app=\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m, master=\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    353\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m created by \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m at \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    354\u001b[0m         \u001b[38;5;241m%\u001b[39m (currentAppName, currentMaster,\n\u001b[1;32m    355\u001b[0m             callsite\u001b[38;5;241m.\u001b[39mfunction, callsite\u001b[38;5;241m.\u001b[39mfile, callsite\u001b[38;5;241m.\u001b[39mlinenum))\n\u001b[1;32m    356\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    357\u001b[0m     SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;241m=\u001b[39m instance\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=PEFIM, master=local[*]) created by __init__ at /tmp/ipykernel_69/461506957.py:4 "
     ]
    }
   ],
   "source": [
    "# variables used in the algorithm\n",
    "APP_NAME = \"PEFIM\"\n",
    "conf = SparkConf().setAppName(APP_NAME)\n",
    "sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "579b48e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputfile = 'thesisDatabase.txt'\n",
    "numPartitions = 4\n",
    "minUtil = 50\n",
    "partitionType = 'lookup'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2dfc6001",
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildTransaction(line):\n",
    "#     gets a input line and builds a transaction with the line\n",
    "    line = line.strip().split(':')\n",
    "    items = line[0].strip().split(' ')\n",
    "    items = [int(item) for item in items]\n",
    "    twu = float(line[1])\n",
    "    utilities = line[2].strip().split(' ')\n",
    "    utilities = [float(utility) for utility in utilities]\n",
    "    # creating a transaction\n",
    "    transaction = Transaction(items, utilities, twu)\n",
    "    return transaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "01334a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFileStats(transactions):\n",
    "    transactionUtilities = transactions.flatMap(lambda x: [x.getTransactionUtility()])\n",
    "    totalutility = transactionUtilities.reduce(add)\n",
    "    datasetLen = len(transactionUtilities.collect())\n",
    "    return {\n",
    "        'len' : datasetLen,\n",
    "        'totalUtility' : totalutility\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "78a33161",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function not only revises the transaction but also calculates the NSTU value of each secondary item\n",
    "def reviseTransactions(transaction):\n",
    "    transaction.removeUnpromisingItems(oldNamesToNewNames_broadcast.value)\n",
    "    return transaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f1fa5dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculates the subtree utility of secondary items\n",
    "def calculateSTUFirstTime(transaction):\n",
    "    # secondary items\n",
    "    secondaryItems = list(oldNamesToNewNames_broadcast.value.keys())\n",
    "    items = transaction.getItems()\n",
    "    utilities = transaction.getUtilities()\n",
    "    itemsUtilityList = []\n",
    "    sumSU = 0\n",
    "    i = len(items) - 1\n",
    "    while i >= 0:\n",
    "        item = items[i]\n",
    "        sumSU += utilities[i]\n",
    "        itemsUtilityList.append((item, sumSU))\n",
    "        i -= 1\n",
    "    return itemsUtilityList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cba26df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function just collects the transaction and prints the items and utilities present in the transaction\n",
    "def printTransactions(transactions):\n",
    "    for transaction in transactions.collect():\n",
    "        print('transaction start')\n",
    "        print(transaction.getItems())\n",
    "        print(transaction.getUtilities())\n",
    "        print('transaction ends') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ebfbc0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# divides the items between the partitions based on certain techniques\n",
    "def divideItems(items, numPartitions, partitionType):\n",
    "    itemNode = {}\n",
    "    NodeToItemMap = {}\n",
    "    for i in range(numPartitions):\n",
    "        NodeToItemMap[i] = []\n",
    "    if partitionType == 'lookup':\n",
    "        i = 0\n",
    "        inc = 1\n",
    "        flag = False\n",
    "        for item in items:\n",
    "            itemNode[item] = i\n",
    "            NodeToItemMap[i].append(item)\n",
    "            i += inc\n",
    "            if (i == 0) or (i == numPartitions -1):\n",
    "                if flag:\n",
    "                    if i == 0:\n",
    "                        inc = 1\n",
    "                    else:\n",
    "                        inc = -1\n",
    "                    flag = False\n",
    "                else:\n",
    "                    inc = 0\n",
    "                    flag = True\n",
    "    for i in range(numPartitions):\n",
    "        NodeToItemMap[i] = set(NodeToItemMap[i])\n",
    "        \n",
    "    return itemNode, NodeToItemMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e16e371b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def defaultBooleanValue():\n",
    "    return False\n",
    "\n",
    "def mapTransaction(transaction):\n",
    "    items = transaction.getItems()\n",
    "    utilities = transaction.getUtilities()\n",
    "    totalUtility = transaction.getTransactionUtility()\n",
    "    mapItemToNodeID = itemToNodeMap_broadcast.value\n",
    "    mapNodeID = defaultdict(defaultBooleanValue)\n",
    "    transactionList = []\n",
    "    cumulativeUtility = 0\n",
    "    primaryItems = list(mapItemToNodeID.keys())\n",
    "    for idx, item in enumerate(items):\n",
    "        if item not in primaryItems:\n",
    "            cumulativeUtility += utilities[idx]\n",
    "            continue\n",
    "        nodeID = mapItemToNodeID[item]\n",
    "        # if this transaction is not assigned to the node \n",
    "        if not mapNodeID[nodeID]:\n",
    "            # create a new transaction\n",
    "            newTransaction = Transaction(items[idx:], utilities[idx:], totalUtility - cumulativeUtility)\n",
    "            transactionList.append((nodeID, newTransaction))\n",
    "            mapNodeID[nodeID] = True\n",
    "        cumulativeUtility += utilities[idx]\n",
    "    return transactionList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9ccb0752",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[30] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reading the data from the text file and transorfming each line into a transaction\n",
    "transactions = sc.textFile(inputfile, numPartitions).map(lambda x : buildTransaction(x))\n",
    "transactions.persist()\n",
    "\n",
    "# compute the statistics of the database\n",
    "filestats = getFileStats(transactions)\n",
    "\n",
    "# calculate the TWU value for each item present in the database\n",
    "twuDict = dict(transactions.flatMap(lambda x: [(item, x.getTransactionUtility()) for item in x.getItems()]).reduceByKey(add).filter(lambda x: x[1] >= minUtil).collect())\n",
    "\n",
    "# the keys in the dictionary are the items which we keep in the database we call them as primary items\n",
    "secondaryItems = list(twuDict.keys())\n",
    "\n",
    "# sorting the primary keys in increasing order of their TWU values\n",
    "secondaryItems.sort(key = lambda x: twuDict[x])\n",
    "\n",
    "# give new names to the items based upon their ordering starting from 1\n",
    "oldNamesToNewNames = {} # dictionary for storing the mappings from old names to new names\n",
    "newNamesToOldNames = {} # dictionary to map from new names to old names\n",
    "currentName = 1\n",
    "for idx, item in enumerate(secondaryItems):\n",
    "    oldNamesToNewNames[item] = currentName\n",
    "    newNamesToOldNames[currentName] = item\n",
    "    secondaryItems[idx] = currentName\n",
    "    currentName += 1\n",
    "\n",
    "# broadcasting the oldNamesToNewNames Dictionary which will be used by the transaction to get the revised transaction\n",
    "oldNamesToNewNames_broadcast = sc.broadcast(oldNamesToNewNames)\n",
    "newNamesToOldNames_broadcast = sc.broadcast(newNamesToOldNames)\n",
    "minUtil_broadcast = sc.broadcast(minUtil)\n",
    "\n",
    "# Remove non secondary items from each transaction and sort remaining items in increasing order of their TWU values\n",
    "revisedTransactions = transactions.map(reviseTransactions).filter(lambda x: len(x.getItems()) > 0)\n",
    "revisedTransactions.persist()\n",
    "transactions.unpersist()\n",
    "\n",
    "# Calculate the subtree utility of each item in secondary item\n",
    "STU_dict = dict(revisedTransactions.flatMap(calculateSTUFirstTime).reduceByKey(add).filter(lambda x: x[1] >= minUtil).collect())\n",
    "\n",
    "# primary items or the items which need to be projected in DFS traversal of the search space\n",
    "primaryItems = list(STU_dict.keys())\n",
    "primaryItems.sort(key= lambda x: twuDict[newNamesToOldNames[x]])\n",
    "\n",
    "\n",
    "itemToNodeMap, nodeToItemsMap = divideItems(primaryItems, numPartitions, partitionType)\n",
    "itemToNodeMap_broadcast = sc.broadcast(itemToNodeMap)\n",
    "nodeToItemsMap_broadcast = sc.broadcast(dict(nodeToItemsMap))\n",
    "\n",
    "# creating a new key-value RDD where key is node id and value is list of transactions at that node id\n",
    "partitionTransactions = revisedTransactions.flatMap(mapTransaction).groupByKey().mapValues(list)\n",
    "partitionTransactions.persist()\n",
    "revisedTransactions.unpersist()\n",
    "\n",
    "# repartition the data into nodes depending upon the key\n",
    "# # transactions = transactions.partitionBy(numPartitions, lambda k: int(k[0]))\n",
    "# # partitioner = RangePartitioner(numPartitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c258cacf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parllelEFIM(nodeData):\n",
    "    currNode = nodeData[0]\n",
    "    transactions = nodeData[1]\n",
    "    primaryItems = nodeToItemsMap_broadcast.value\n",
    "    primaryItems = primaryItems[currNode]\n",
    "    minUtil = minUtil_broadcast.value\n",
    "    oldNamesToNewNames = oldNamesToNewNames_broadcast.value\n",
    "    newNamesToOldNames = newNamesToOldNames_broadcast.value\n",
    "    secondaryItems = list(newNamesToOldNames.keys())\n",
    "    pefim = pEFIM(minUtil, primaryItems, secondaryItems, transactions, newNamesToOldNames, oldNamesToNewNames)\n",
    "    output = pefim.runAlgo()\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9b5fd751",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for idx, transaction in enumerate(partitionTransactions.collect()):\n",
    "#     if idx == 1:\n",
    "#         parllelEFIM(transaction)\n",
    "    \n",
    "huis = partitionTransactions.map(parllelEFIM).groupByKey().map(lambda x : (x[0], list(x[1]))).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0c3f3db4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n"
     ]
    }
   ],
   "source": [
    "itemsets = [y for x in huis[0][1] if len(x) > 0 for y in x]\n",
    "print(len(itemsets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8207c86-571e-4ee3-96bd-012fd1c3eb37",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
